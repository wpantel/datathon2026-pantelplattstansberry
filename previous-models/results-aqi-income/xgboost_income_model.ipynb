{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost: Predicting AQI from Income\n",
    "\n",
    "This notebook trains an XGBoost model to predict median AQI (livability proxy) from median household income. Part of a larger analysis showing how socioeconomic factors correlate with access to livable climate.\n",
    "\n",
    "- **Target:** median_aqi\n",
    "- **Feature:** Median_Household_Income\n",
    "- **Stratified 80/20 split** by AQI quartiles\n",
    "- **Sample weights** to downweight low-quality observations\n",
    "- **Hyperparameter tuning** via stratified 5-fold CV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Data and Prepare "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape: (940, 6)\n",
      "median_aqi range: 3 - 90\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>State</th>\n",
       "      <th>County</th>\n",
       "      <th>Year</th>\n",
       "      <th>median_aqi</th>\n",
       "      <th>sample_weight</th>\n",
       "      <th>Median_Household_Income</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Alabama</td>\n",
       "      <td>Baldwin</td>\n",
       "      <td>2025</td>\n",
       "      <td>42</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>78775.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Alabama</td>\n",
       "      <td>Clay</td>\n",
       "      <td>2025</td>\n",
       "      <td>32</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>55250.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Alabama</td>\n",
       "      <td>DeKalb</td>\n",
       "      <td>2025</td>\n",
       "      <td>42</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>51204.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Alabama</td>\n",
       "      <td>Elmore</td>\n",
       "      <td>2025</td>\n",
       "      <td>32</td>\n",
       "      <td>0.983333</td>\n",
       "      <td>78243.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Alabama</td>\n",
       "      <td>Etowah</td>\n",
       "      <td>2025</td>\n",
       "      <td>45</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>54563.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     State   County  Year  median_aqi  sample_weight  Median_Household_Income\n",
       "0  Alabama  Baldwin  2025          42       1.000000                  78775.0\n",
       "1  Alabama     Clay  2025          32       1.000000                  55250.0\n",
       "2  Alabama   DeKalb  2025          42       1.000000                  51204.0\n",
       "3  Alabama   Elmore  2025          32       0.983333                  78243.0\n",
       "4  Alabama   Etowah  2025          45       1.000000                  54563.0"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import xgboost as xgb\n",
    "\n",
    "df = pd.read_csv('../JOINED-aqi-income/aqi-income-joined.csv')\n",
    "df = df.dropna(subset=['median_aqi', 'Median_Household_Income', 'sample_weight'])\n",
    "\n",
    "print(f\"Data shape: {df.shape}\")\n",
    "print(f\"median_aqi range: {df['median_aqi'].min():.0f} - {df['median_aqi'].max():.0f}\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Create Stratification Bins and Train/Test Split\n",
    "\n",
    "We stratify by AQI quartiles so train and test sets have similar distributions of livability levels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stratum distribution:\n",
      "aqi_stratum\n",
      "Q1    262\n",
      "Q2    209\n",
      "Q3    253\n",
      "Q4    216\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Train: 752, Test: 188\n"
     ]
    }
   ],
   "source": [
    "# Quartile-based stratification (data is mostly 0-50 AQI, quartiles give balanced groups)\n",
    "df['aqi_stratum'] = pd.qcut(df['median_aqi'], q=4, labels=['Q1', 'Q2', 'Q3', 'Q4'], duplicates='drop')\n",
    "print(\"Stratum distribution:\")\n",
    "print(df['aqi_stratum'].value_counts().sort_index())\n",
    "\n",
    "X = df[['Median_Household_Income']]\n",
    "y = df['median_aqi']\n",
    "weights = df['sample_weight']\n",
    "strata = df['aqi_stratum']\n",
    "\n",
    "X_train, X_test, y_train, y_test, w_train, w_test, strata_train, strata_test = train_test_split(\n",
    "    X, y, weights, strata, test_size=0.2, random_state=42, stratify=strata\n",
    ")\n",
    "\n",
    "print(f\"\\nTrain: {len(X_train)}, Test: {len(X_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Hyperparameter Tuning with Stratified CV\n",
    "\n",
    "We use stratified 5-fold CV and a search over common XGBoost hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 36 candidates, totalling 180 fits\n",
      "Best params: {'learning_rate': 0.05, 'max_depth': 3, 'min_child_weight': 3, 'n_estimators': 100}\n",
      "Best CV MSE: 95.86677004106876\n"
     ]
    }
   ],
   "source": [
    "# Precompute stratified CV splits (stratify on AQI quartiles)\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "cv_splits = list(skf.split(X_train, strata_train))\n",
    "\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [3, 5, 7],\n",
    "    'learning_rate': [0.05, 0.1],\n",
    "    'min_child_weight': [1, 3],\n",
    "}\n",
    "\n",
    "xgb_model = xgb.XGBRegressor(random_state=42, objective='reg:squarederror')\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    xgb_model, param_grid, cv=cv_splits, scoring='neg_mean_squared_error',\n",
    "    n_jobs=-1, verbose=1\n",
    ")\n",
    "\n",
    "# sample_weight passed to fit(); XGBRegressor uses it to downweight low-quality observations\n",
    "grid_search.fit(X_train, y_train, sample_weight=w_train)\n",
    "\n",
    "print(\"Best params:\", grid_search.best_params_)\n",
    "print(\"Best CV MSE:\", -grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Train Final Model and Evaluate on Test Set\n",
    "\n",
    "Refit the best model on the full training set (with sample weights) and evaluate on held-out test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set metrics:\n",
      "  RMSE: 10.42\n",
      "  MAE:  6.93\n",
      "  R²:   -0.0375\n"
     ]
    }
   ],
   "source": [
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "# Refit on full training set with sample weights\n",
    "best_model.fit(X_train, y_train, sample_weight=w_train)\n",
    "\n",
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "print(\"Test set metrics:\")\n",
    "print(f\"  RMSE: {np.sqrt(mean_squared_error(y_test, y_pred)):.2f}\")\n",
    "print(f\"  MAE:  {mean_absolute_error(y_test, y_pred):.2f}\")\n",
    "print(f\"  R²:   {r2_score(y_test, y_pred):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Feature Importance\n",
    "\n",
    "With a single feature (income), importance shows how much it contributes to predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   feature  importance\n",
      "0  Median_Household_Income         1.0\n"
     ]
    }
   ],
   "source": [
    "importance = pd.DataFrame({\n",
    "    'feature': X_train.columns,\n",
    "    'importance': best_model.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(importance)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
